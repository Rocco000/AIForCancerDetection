{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM3/Lx4YW/at6dM541h8xgS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rocco000/OncoVision/blob/main/Scripts/TrainModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import the dataset script**"
      ],
      "metadata": {
        "id": "mOlcH8s8kjMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #Connect to Google Drive\n",
        "\n",
        "#Run the .ipynb file\n",
        "%run '/content/drive/MyDrive/Colab Notebooks/DatasetLoader.ipynb'\n",
        "%run '/content/drive/MyDrive/Colab Notebooks/ModelArchitecture1.ipynb'\n",
        "\n",
        "#Now I can access to the methods of this file\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \",device)"
      ],
      "metadata": {
        "id": "jLBCSvCNkdfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script to EVALUATE the model"
      ],
      "metadata": {
        "id": "hcKWExlJaS8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(model, data_loader, decision):\n",
        "  #Set the model in evaluation mode\n",
        "  print(\"Start VALIDATION STEP\")\n",
        "  model.eval()\n",
        "  true_labels = []\n",
        "  predicted_labels = []\n",
        "\n",
        "  #In this way we don't computing the gradient\n",
        "  with torch.no_grad():\n",
        "    for images, labels in data_loader:\n",
        "      #Move the image on gpu or cpu. It depends by device variable\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      #Provide the samples to the model\n",
        "      predictions = model(images)\n",
        "\n",
        "      #Apply the Softmax activation function. Dim=1 because the output size is [64,2] where the model prediction is in the second column\n",
        "      predictions = F.softmax(predictions, dim=1)\n",
        "\n",
        "      #To extract the predicted class with the highest probability for each input sample. 1 to indicate on which dimension apply the max\n",
        "      _, predictions = torch.max(predictions, 1)\n",
        "\n",
        "      #print(\"True labels:\")\n",
        "      #print(labels)\n",
        "      #print(\"Predicted labels:\")\n",
        "      #print(predictions)\n",
        "\n",
        "\n",
        "      #Store the true labels and the predicted labels\n",
        "      true_labels.extend(labels.cpu().numpy())\n",
        "      predicted_labels.extend(predictions.cpu().numpy())\n",
        "      break\n",
        "\n",
        "  #Computing the EVALUATION METRICS\n",
        "  #.cpu() because it can't convert cuda:0 device type tensor to numpy.\n",
        "  true_labels = np.array(true_labels)\n",
        "  predicted_labels = np.array(predicted_labels)\n",
        "\n",
        "  accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "  precision = precision_score(true_labels, predicted_labels)\n",
        "  recall = recall_score(true_labels, predicted_labels)\n",
        "  f1 = f1_score(true_labels, predicted_labels)\n",
        "\n",
        "  if decision == 1:\n",
        "    print(\"Evaluation metrics in validation set:\")\n",
        "  else:\n",
        "    print(\"Evaluation metrics in test set:\")\n",
        "  print(\"Accuracy: \",accuracy, \"; Precision: \",precision, \"; Recall: \",recall,\"; F1: \",f1)\n",
        "\n",
        "  cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "  tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "  if decision == 1:\n",
        "    return accuracy, recall\n",
        "  else:\n",
        "    return cm, tp, tn, fp, fn, accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "YCtq_ODtZLs9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script to TRAIN the model"
      ],
      "metadata": {
        "id": "T8Oo1M7TaXzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import transforms\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow #because Google Colab doesn't support cv2.imshow(), it causes Jupyter session to crash\n",
        "import cv2\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "\n",
        "#TRAIN STEP\n",
        "def train_model(model, data_loader, valid_loader, num_epoch, criterion, optimizer):\n",
        "  print(\"START TRAINING STEP\")\n",
        "  model.train()\n",
        "  patience = 0 #We use it to verify if the model not improve\n",
        "  best_recall = 0\n",
        "  best_accuracy = 0\n",
        "  best_loss = 0\n",
        "  best_epoch = 0\n",
        "\n",
        "  model_state = None #To store the best model status\n",
        "  optimizer_state = None #To store the best optimizer status\n",
        "\n",
        "  #To improve the image sharpness. It is random, defined by the parameter P. With sharpness_factor = 1 we only improve the image sharpness\n",
        "  up_sharpness = torchvision.transforms.RandomAdjustSharpness(sharpness_factor=1.5, p=0.5)\n",
        "\n",
        "  #To improve the image contrast. It is random, defined by the parameter P\n",
        "  up_contrast = torchvision.transforms.RandomAutocontrast(p=0.5)\n",
        "\n",
        "  for e in range(num_epoch):\n",
        "    for images, labels in data_loader:\n",
        "      model.zero_grad() #Set the gradient to zero for each batch\n",
        "\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      #Improve the image sharpness and the image contrast\n",
        "      images = up_sharpness(images)\n",
        "      images = up_contrast(images)\n",
        "\n",
        "      #FORWARD PASS\n",
        "      predictions = model(images)\n",
        "      #predictions = predictions.unsqueeze(1)\n",
        "      #print(\"Shape dopo unsqueeze: \", predictions.shape)\n",
        "\n",
        "      #Measure the loss function\n",
        "      loss = criterion(predictions, labels)\n",
        "\n",
        "      #print(\"True label:\")\n",
        "      #print(labels)\n",
        "      #print(\"Predicted label:\")\n",
        "      #print(torch.max(F.softmax(predictions, dim=1), 1)[1])\n",
        "\n",
        "      #BACKWARD PASS\n",
        "      optimizer.zero_grad() #set the gradient to 0\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    #After trained the model on the batchs, we test the model on the VALIDATION SET\n",
        "    #Test the model on the VALIDATION SET\n",
        "    accuracy, recall = evaluate_model(model, valid_loader, 1)\n",
        "    if recall == best_recall and accuracy == best_accuracy:\n",
        "      patience = patience+1\n",
        "    elif recall>best_recall and accuracy==best_accuracy:\n",
        "      patient = 0\n",
        "      best_recall = recall\n",
        "      best_loss = loss.item()\n",
        "      model_state = model.state_dict()\n",
        "      optimizer_state = optimizer.state_dict()\n",
        "      best_epoch = e\n",
        "    elif recall == best_recall and accuracy>best_accuracy:\n",
        "      patient = 0\n",
        "      best_accuracy = accuracy\n",
        "      best_loss = loss.item()\n",
        "      model_state = model.state_dict()\n",
        "      optimizer_state = optimizer.state_dict()\n",
        "      best_epoch = e\n",
        "    elif recall>best_recall and accuracy>best_accuracy:\n",
        "      patient = 0\n",
        "      best_accuracy = accuracy\n",
        "      best_recall = recall\n",
        "      best_loss = loss.item()\n",
        "      model_state = model.state_dict()\n",
        "      optimizer_state = optimizer.state_dict()\n",
        "      best_epoch = e\n",
        "    else:\n",
        "      patience = patience+1\n",
        "\n",
        "    model.train()\n",
        "    if patience>32:\n",
        "      print(\"Train step stopped at epoch \",e+1,\" because the model doesn't improve!\")\n",
        "      if model_state is not None and optimizer_state is not None:\n",
        "        model.load_state_dict(model_state) #Set the model with the best configuration\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "      break\n",
        "    print(f\"Epoch [{e+1}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "  print(\"The best model and optimizer configuration was achieved at the \",best_epoch,\" epoch\")\n",
        "  print(\"The best accuracy: \",best_accuracy,\" best recall: \",best_recall, \"best loss: \",best_loss)\n",
        "  print(\"Finished Training!\")\n",
        "  return model_state, optimizer_state\n"
      ],
      "metadata": {
        "id": "C7Zgwa0qZbbx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAIN THE MODEL**"
      ],
      "metadata": {
        "id": "vxQX-Xbsacex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_benign = \"/content/drive/MyDrive/SE4AI/Data/Datasets/Dataset1/benign\"\n",
        "path_malignant = \"/content/drive/MyDrive/SE4AI/Data/Datasets/Dataset1/malignant\"\n",
        "\n",
        "#Get the dataloader for each set\n",
        "train_loader, valid_loader, test_loader = get_dataset(\"/content/drive/MyDrive/SE4AI/Data/Datasets/Dataset1/\",percent_train=0.7, percent_valid=0.2, batch_size=64)\n",
        "\n",
        "#Plot the samples for each set\n",
        "plot_samples(train_loader, \"TRAIN SET\")\n",
        "plot_samples(test_loader, \"TEST SET\")\n",
        "plot_samples(valid_loader, \"VALID SET\")\n",
        "\n",
        "#Define the CNN\n",
        "model = ConvModel1().to(device)\n",
        "\n",
        "#Define the loss function -> CrossEntropyLoss because i build a cnn for binary classification using two neurons in the output layer that represent the corresponding class\n",
        "#This loss already apply the Softmax activation function in order to ensure that the probabilities sum up to 1.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Define the optimizer\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr= 0.001)\n",
        "\n",
        "#Start the train step\n",
        "best_model_configuration, best_optimizer_configuration = train_model(model=model, data_loader=train_loader, valid_loader=valid_loader, num_epoch= 64, criterion=criterion, optimizer=optimizer)\n",
        "\n",
        "#Store the best configurations\n",
        "torch.save(best_model_configuration, '/content/drive/MyDrive/SE4AI/Model/EvaluationOnDB1NoCleaned/model_parameters.pth')\n",
        "torch.save(best_optimizer_configuration, '/content/drive/MyDrive/SE4AI/Model/EvaluationOnDB1NoCleaned/optimizer_parameters.pth')"
      ],
      "metadata": {
        "id": "pJaq5EduGGHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EVALUATE THE MODEL**"
      ],
      "metadata": {
        "id": "0LkT158MhZ79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "\n",
        "model.load_state_dict(best_model_configuration) #Load the best model configuration\n",
        "model.to(device)\n",
        "\n",
        "confusion, tp, tn, fp, fn, accuracy, precision, recall, f1 = evaluate_model(model, test_loader, 0)\n",
        "#Create a heatmap of the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "\n",
        "#Save the confusion metrix\n",
        "plt.savefig('/content/drive/MyDrive/SE4AI/Model/EvaluationOnDB1NoCleaned/confusion_matrix.png')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"TP: \",tp,\" TN: \",tn,\" FP: \",fp,\" FN:\",fn)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/SE4AI/Model/EvaluationOnDB1NoCleaned/evaluation_metrics.csv\", \"w\", newline=\"\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"Accuracy\",\"Precision\",\"Recall\",\"F1-Score\"])\n",
        "    writer.writerow([accuracy,precision,recall,f1])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sJSLj6mkgrOX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}